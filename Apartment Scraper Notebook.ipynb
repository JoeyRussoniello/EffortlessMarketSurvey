{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To delete all instances of chromedriver for cleaning is: taskkill /F /IM chromedriver.exe /T\n",
    "\n",
    "#Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "# Format the date as YYYY MM DD\n",
    "formatted_date = today.strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Scraper functions\n",
    "def not_containing(l, filterchar):\n",
    "    return list(filter(lambda text: text != filterchar,l))\n",
    "def get_comps():\n",
    "    comps = pd.read_csv(r\"C:\\Users\\JosephRussoniello\\OneDrive - Red Tail Residential\\Python\\Apartment Scrape Python Project\\.venv\\Inputs\\Property Comps.csv\")\n",
    "    areas = {}\n",
    "    for (index, row) in comps.iterrows():\n",
    "        areas[row[\"Address\"]] = row[\"Property\"]\n",
    "    return areas\n",
    "def initializeDriver(headless=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        headless (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    #Opens a new chrome driver in headless mode, or regularly\n",
    "    service =  Service(executable_path=r\"C:\\Users\\JosephRussoniello\\OneDrive - Red Tail Residential\\Python\\Apartment Scrape Python Project\\.venv\\chromedriver.exe\")\n",
    "    if headless == False:\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "    else:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\")\n",
    "        driver = webdriver.Chrome(service=service,options= options)\n",
    "    return driver\n",
    "def findArea(driver,area):\n",
    "    \"\"\"Searches an <area> on apartments.com given an active WebDriver\"\"\"\n",
    "    print(f\"Beginning survey for {area}...\")\n",
    "    #Open base apartments.com\n",
    "    driver.get(\"https://www.apartments.com/\")\n",
    "    #Wait until page loads\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"quickSearchLookup\")))\n",
    "\n",
    "    #Enter words into search area\n",
    "    area_input = driver.find_element(By.ID,\"quickSearchLookup\")\n",
    "    area_input.send_keys(area)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    #Click button to submit requiest\n",
    "    button = driver.find_element(By.CSS_SELECTOR, \"button[title='Search apartments for rent']\")\n",
    "    button.click()\n",
    "\n",
    "    #Wait until page is found, precautionary measure to future functions\n",
    "    try:\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.property-link\")))\n",
    "    except:\n",
    "        TimeoutError\n",
    "def getLinks(driver):\n",
    "    #Get all competitive properties in the area, given the driver is open on the area\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"a.property-link\")\n",
    "    hrefs = [link.get_attribute(\"href\") for link in links]\n",
    "    return hrefs\n",
    "def unique(seq):\n",
    "    #Helper that gets all unique values without modifying order. Used with getLinks\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "def getCompData(driver, rtprop,verbose = False):\n",
    "    properties = []\n",
    "    #Iterate over all links\n",
    "    #Wait until page loads\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.propertyName\")))\n",
    "    except TimeoutError:\n",
    "        return\n",
    "    \n",
    "    #Find details\n",
    "    prop = driver.find_element(By.CSS_SELECTOR,\"h1.propertyName\").text\n",
    "    if verbose == True:\n",
    "        print(f\"Gathering data for {prop}...\")\n",
    "    \n",
    "    prop_info = driver.find_elements(By.CSS_SELECTOR, \"li.unitContainer.js-unitContainer\")\n",
    "\n",
    "    divs = driver.find_elements(By.CSS_SELECTOR,\"div.priceBedRangeInfo\")\n",
    "\n",
    "    #Bro just learned list comprehension\n",
    "    plans = [div.find_element(By.CSS_SELECTOR, \"span.modelName\").text for div in divs]\n",
    "    details = [div.find_element(By.CSS_SELECTOR, \"h4.detailsLabel\").text for div in divs]\n",
    "\n",
    "\n",
    "    #Remove extra blank entries\n",
    "    plans = not_containing(plans,\"\")\n",
    "    details = not_containing(details,\"\")\n",
    "\n",
    "    #Disgusting list comprehensions to get values from details\n",
    "    sqft = []\n",
    "    deposits = []\n",
    "    for detail in details:\n",
    "        if \"deposit\" in detail:\n",
    "            sqft.append(\"\".join(detail.partition(\"\\n\")[0].split(\",\")[2:]).strip())\n",
    "            deposits.append(detail.split(\"\\n\")[1].partition(\"deposit\")[0] + \"deposit\")\n",
    "        elif \"sq ft\" in detail:\n",
    "            sqft.append(\"\".join(detail.split(\",\")[2:]).strip())\n",
    "            deposits.append(\"\")\n",
    "        else:\n",
    "            sqft.append(\"\")\n",
    "            deposits.append(\"\")\n",
    "\n",
    "    modeldict = {plans[i]: [sqft[i],deposits[i]] for i in range(len(plans))}\n",
    "\n",
    "    cols = [\"pricing\",\"sqft\",\"available\",\"unit\"]\n",
    "#\"\"\"\n",
    "    for item in prop_info:\n",
    "        data_model = item.get_attribute(\"data-model\")\n",
    "        if data_model in modeldict:\n",
    "            attributes = {\n",
    "                \"ID\":prop + \" \" + data_model,\n",
    "                \"Area\":rtprop,\n",
    "                \"Property\":prop,\n",
    "                \"Beds\": item.get_attribute(\"data-beds\"),\n",
    "                \"Baths\": item.get_attribute(\"data-baths\"),\n",
    "                \"Model\": data_model,\n",
    "                \"Security Deposit\": modeldict[data_model][1],\n",
    "                \"Report Date\":formatted_date,\n",
    "            }\n",
    "            for col in cols:\n",
    "                nicercol = col[0].upper() + col[1:]\n",
    "                #Unavailable units trip this up, so break if we find one\n",
    "                try:\n",
    "                    attributes[nicercol] = item.find_element(By.CSS_SELECTOR,f\"div.{col}Column.column\").text.split(\"\\n\")[1]\n",
    "                except IndexError:\n",
    "                    if (properties == None or properties == []) and verbose == True:\n",
    "                        print(\"     No data found\")\n",
    "                    return properties\n",
    "            properties.append(attributes)\n",
    "    if (properties == None or properties == []) and verbose == True:\n",
    "        print(\"     No data found\")\n",
    "    return properties\n",
    "def getAreaData(driver, area, rt_prop,ncomps = 5,verbose = False):\n",
    "    #Search for the area on apartments.com\n",
    "    findArea(driver,area)\n",
    "    #Get the links for the comps in the area\n",
    "    hrefs = getLinks(driver)\n",
    "    #Make sure link list is unique\n",
    "    hrefs = unique(hrefs)\n",
    "    if verbose == True:\n",
    "        print(f\"Found links: {hrefs}...\")\n",
    "\n",
    "    properties = []\n",
    "    numfound = 0\n",
    "    #Get as many links as possible with data, up to ncomps\n",
    "    for href in hrefs:\n",
    "        if numfound == ncomps:\n",
    "            break\n",
    "        hrefproperties = getCompData(driver,href,area,rt_prop,verbose)\n",
    "        if hrefproperties != None and hrefproperties != []:\n",
    "            properties.extend(hrefproperties)\n",
    "            numfound += 1\n",
    "    return properties\n",
    "\n",
    "def newMarketSurvey(areas_dict,headless=False,verbose=True):\n",
    "    driver = initializeDriver(headless=headless)\n",
    "    data = []\n",
    "    for address,rtprop in areas_dict.items():\n",
    "        findArea(driver,address)\n",
    "        try:\n",
    "            properties = getCompData(driver=driver,rtprop=rtprop,verbose=verbose)\n",
    "            data.extend(properties)\n",
    "        except TimeoutException:\n",
    "            if verbose == True:\n",
    "                print(f\"Invalid prop {address}\")\n",
    "            pass\n",
    "    driver.quit()\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "def clean_df(df):\n",
    "    df.set_index(\"ID\",inplace=True)\n",
    "    for i in df.columns:\n",
    "        df[i][df[i].apply(lambda i: True if re.search('^\\\\s*$', str(i)) else False)]=None\n",
    "    df = df[df[\"Pricing\"].str[0] == \"$\"]\n",
    "    #Security Deposit\n",
    "    deposit = df[\"Security Deposit\"]\n",
    "    numified_sd = deposit.apply(lambda x: x.partition(\" deposit\")[0][1:].replace(\",\",\"\") if type(x) == str else x).astype(float)\n",
    "    df[\"Security Deposit\"] = numified_sd\n",
    "\n",
    "    #Listed Rent \n",
    "    rents = df[\"Pricing\"]\n",
    "    numified_rent = rents.apply(lambda x: x[1:].replace(\",\",\"\")).astype(float)\n",
    "    df[\"Pricing\"] = numified_rent\n",
    "\n",
    "    #Square feet\n",
    "    df[\"Sqft\"] = df[\"Sqft\"].apply(lambda x: x.replace(\",\",\"\")).astype(int)\n",
    "    #Sorry future me, this function is really gross, but just concatenates and shortens the Beds/Baths columns\n",
    "    df[\"Bed/Baths\"] = df[\"Beds\"].astype(str) + \" Bed \" + df[\"Baths\"].apply(lambda x: int(x) if pd.notnull(x) and isinstance(x, (int, float)) and x % 1 == 0 else x).astype(str) + \" Bath\"\n",
    "    return df\n",
    "def group_df(df):\n",
    "    df_excluded = df.drop(columns = [\"Beds\",\"Baths\"])\n",
    "    grouped = df_excluded.groupby(['Area','Property','Bed/Baths','Report Date']).mean(numeric_only=True).round(2)\n",
    "    #grouped.to_csv(\"Outputs\\\\Pivot.csv\")\n",
    "    return grouped\n",
    "def write_df(df,path,mode):\n",
    "    if mode == \"a\":\n",
    "        df.to_csv(path,mode=mode,header=False)\n",
    "    else:\n",
    "        df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = get_comps()\n",
    "df = newMarketSurvey(areas,headless=True,verbose=True)\n",
    "cleaned = clean_df(df)\n",
    "grouped = group_df(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grouped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m write_df(\u001b[43mgrouped\u001b[49m,\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJosephRussoniello\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - Red Tail Residential\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mApartment Scrape Python Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.venv\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOutputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMarket Survey Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grouped' is not defined"
     ]
    }
   ],
   "source": [
    "write_df(grouped,r'C:\\Users\\JosephRussoniello\\OneDrive - Red Tail Residential\\Python\\Apartment Scrape Python Project\\.venv\\Outputs\\Market Survey Data.csv',mode='o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
